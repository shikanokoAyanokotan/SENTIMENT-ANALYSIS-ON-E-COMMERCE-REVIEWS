{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch scikit-learn gdown"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fd9EciVmmgKV",
        "outputId": "b113d32c-eddd-4a15-f1f9-0686db700a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.11/dist-packages (5.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown) (2.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown) (1.7.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gdown\n",
        "import re\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, EarlyStoppingCallback\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Mapping labels\n",
        "label_mapping = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
        "\n",
        "# Function to download file from Google Drive\n",
        "def download_gdrive_file(file_name, drive_link):\n",
        "  if not os.path.exists(file_name):\n",
        "    # Convert view link to download link\n",
        "    file_id = drive_link.split('/d/')[1].split('/')[0]\n",
        "    download_url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "    print(f\"Downloading {file_name} from {download_url} ...\")\n",
        "    gdown.download(download_url, file_name, quiet=False)\n",
        "  else:\n",
        "    print(f\"File {file_name} existed.\")"
      ],
      "metadata": {
        "id": "hIAPw7HLx_WL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load train data file\n",
        "def load_train_data(file_name, drive_link):\n",
        "  download_gdrive_file(file_name, drive_link)\n",
        "  with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    texts = f.read()\n",
        "    lines = re.split(r\"\\n+\", texts)\n",
        "  return [line.strip() for line in lines if line.strip()]"
      ],
      "metadata": {
        "id": "Ok7TNOh20WxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load test data file\n",
        "def load_test_data(file_name, drive_link):\n",
        "  download_gdrive_file(file_name, drive_link)\n",
        "  with open(file_name, \"r\", encoding=\"utf-8\") as f:\n",
        "    texts = f.read()\n",
        "    lines = re.split(r\"\\n+\", texts)\n",
        "    lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "  texts = []\n",
        "  labels = []\n",
        "  if len(lines) % 2 != 0:\n",
        "    print(\"Warning: The number of lines in the test file is not even, check the file test again!\")\n",
        "\n",
        "  for i in range(0, len(lines) - 1, 2):\n",
        "    texts.append(lines[i])\n",
        "    label_str = lines[i + 1].upper()\n",
        "    if label_str == \"POS\":\n",
        "      labels.append(2)\n",
        "    elif label_str == \"NEU\":\n",
        "      labels.append(1)\n",
        "    elif label_str == \"NEG\":\n",
        "      labels.append(0)\n",
        "    else:\n",
        "      raise ValueError(f\"Invalid label: {label_str}\")\n",
        "\n",
        "  return texts, labels"
      ],
      "metadata": {
        "id": "Y8Wbmgw909JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(texts):\n",
        "    cleaned_texts = []\n",
        "    for text in texts:\n",
        "        # Lo·∫°i b·ªè URL (v√≠ d·ª•: http://... ho·∫∑c https://...)\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a (bao g·ªìm newline, tab, ...)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "        # X√≥a kho·∫£ng tr·∫Øng ƒë·∫ßu v√† cu·ªëi c√¢u\n",
        "        text = text.strip()\n",
        "        cleaned_texts.append(text)\n",
        "    return cleaned_texts"
      ],
      "metadata": {
        "id": "FN0okHXwzf6l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom dataset\n",
        "class SentimentDataset(Dataset):\n",
        "  def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "    self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length)\n",
        "    self.labels = labels\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "    if self.labels is not None:\n",
        "        item[\"labels\"] = torch.tensor(self.labels[idx])\n",
        "    return item\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.encodings[\"input_ids\"])"
      ],
      "metadata": {
        "id": "7ZvtPhuo3D6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H√†m t√≠nh c√°c ch·ªâ s·ªë ƒë√°nh gi√° (s·ª≠ d·ª•ng khi t·∫≠p test c√≥ ground truth)\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}"
      ],
      "metadata": {
        "id": "KxSDXtJOV2PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê∆∞·ªùng d·∫´n file tr√™n Google Drive\n",
        "train_positive_url = \"https://drive.google.com/file/d/1ufy3fwnrh8XVeKOjSfo0QEvg6YD85RGu/view?usp=sharing\"\n",
        "train_neutral_url  = \"https://drive.google.com/file/d/1RoRvqXwdvdVAcbnoRZgq6r2YYZk9lAmN/view?usp=sharing\"\n",
        "train_negative_url = \"https://drive.google.com/file/d/11GHXGEE5X6QcL1llx4bmYa5hid8Bb8Ja/view?usp=sharing\"\n",
        "test_url           = \"https://drive.google.com/file/d/1pudi3cTGxqs85RopswaQu1panBhjB-wG/view?usp=sharing\"\n",
        "\n",
        "train_negative = load_train_data(\"train_negative_tokenized.txt\", train_negative_url)\n",
        "train_neutral  = load_train_data(\"train_neutral_tokenized.txt\", train_neutral_url)\n",
        "train_positive = load_train_data(\"train_positive_tokenized.txt\", train_positive_url)\n",
        "\n",
        "# G√°n nh√£n: negative -> 0, neutral -> 1, positive -> 2\n",
        "texts_train = train_negative + train_neutral + train_positive\n",
        "texts_train = clean_text(texts_train)\n",
        "labels_train = [0] * len(train_negative) + [1] * len(train_neutral) + [2] * len(train_positive)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "texts_train, texts_val, labels_train, labels_val = train_test_split(texts_train, labels_train, test_size=0.2, stratify=labels_train, random_state=42)\n",
        "\n",
        "print(f\"S·ªë m·∫´u train: {len(texts_train)}\")\n",
        "print(f\"S·ªë m·∫´u validation: {len(texts_val)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UmyqRG7dV6-8",
        "outputId": "16cb338f-6626-45a0-e535-5a02688d92e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File train_negative_tokenized.txt existed.\n",
            "File train_neutral_tokenized.txt existed.\n",
            "File train_positive_tokenized.txt existed.\n",
            "S·ªë m·∫´u train: 4080\n",
            "S·ªë m·∫´u validation: 1020\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts_test, labels_test = load_test_data(\"test_tokenized_ANS.txt\", test_url)\n",
        "texts_test = clean_text(texts_test)\n",
        "\n",
        "print(f\"S·ªë m·∫´u test: {len(texts_test)}\")\n",
        "print(\"V√≠ d·ª• test:\")\n",
        "for i in range(min(3, len(texts_test))):\n",
        "    print(\"Text:\", texts_test[i])\n",
        "    print(\"Label:\", labels_test[i], \"-\", label_mapping[labels_test[i]])\n",
        "    print(\"-----\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usO5jFKPWi5t",
        "outputId": "2a140c9b-5603-4d1b-a7a8-76288e9d41a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File test_tokenized_ANS.txt existed.\n",
            "S·ªë m·∫´u test: 1050\n",
            "V√≠ d·ª• test:\n",
            "Text: Kh√¥ng n√™n mua chu·ªôt cua Logitech , v√¨ d√πng n√≥ r·∫•t kh√≥ ƒë·ªïi c√°i m·ªõi . M√¨nh nghe th·∫±ng b·∫°n x√∫i mua con M325 c√°ch ƒë√¢y 5 nƒÉm , d√π c√≥ c∆°_s·ªë l·∫ßn r∆°i_r·ªõt quƒÉng_qu·∫≠t m√† ƒë·∫øn gi·ªù v·∫´n ch∆∞a h∆∞ . Gi·ªù ƒëang th√®m em MX_Anywhere_2 n√†y m√† chu·ªôt c≈© ch∆∞a h∆∞ sao mua chu·ªôt m·ªõi !\n",
            "Label: 2 - positive\n",
            "-----\n",
            "Text: N√≥i thi·ªát l√† m√¨nh th√¨ th√¨ chu·ªôt n√†o m√¨nh c≈©ng ch∆°i tu·ªët , ch·ªâ tr·ª´ 1 h√£ng ra : Razer . M√¨nh ƒëang s·ªü_h·ªØu 1 con DA black , x√†i ƒë∆∞·ª£c 6 th√°ng n√≥ b·ªã double click , ƒëem s·ª≠a xong x√†i ƒë∆∞·ª£c them 2 th√°ng n·ªØa n√≥ b·ªã h∆∞ n√∫t cu·ªôn ... Trong khi con SS_Sensei m√¨nh x√†i 3 nƒÉm m·ªõi b·ªã double click v√† r√≠t n√∫t cu·ªôn .\n",
            "Label: 0 - negative\n",
            "-----\n",
            "Text: Xai chuot so nhat bi double_click .\n",
            "Label: 1 - neutral\n",
            "-----\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"vinai/phobert-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dJr2rD9WyFU",
        "outputId": "124fd9be-99c2-4d2f-83c3-d2b7037fc031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentimentDataset(texts_train, labels_train, tokenizer)\n",
        "val_dataset   = SentimentDataset(texts_val, labels_val, tokenizer)\n",
        "test_dataset  = SentimentDataset(texts_test, labels_test, tokenizer)"
      ],
      "metadata": {
        "id": "IwrFtpq6W69c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fngc2_HZXDml",
        "outputId": "44199b06-6edf-4a24-ee90-de58150aa502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "L35jVeLsXRwL",
        "outputId": "52d70aba-254a-4b5c-90d6-5bcf47264e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1530' max='2550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1530/2550 11:26 < 07:38, 2.23 it/s, Epoch 6/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.684861</td>\n",
              "      <td>0.709804</td>\n",
              "      <td>0.706344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.782600</td>\n",
              "      <td>0.603152</td>\n",
              "      <td>0.754902</td>\n",
              "      <td>0.753411</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.782600</td>\n",
              "      <td>0.637875</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.749194</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.463100</td>\n",
              "      <td>0.610569</td>\n",
              "      <td>0.772549</td>\n",
              "      <td>0.774113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.463100</td>\n",
              "      <td>0.698304</td>\n",
              "      <td>0.752941</td>\n",
              "      <td>0.748227</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.326400</td>\n",
              "      <td>0.705755</td>\n",
              "      <td>0.759804</td>\n",
              "      <td>0.758483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1530, training_loss=0.5193164943869597, metrics={'train_runtime': 686.9547, 'train_samples_per_second': 59.393, 'train_steps_per_second': 3.712, 'total_flos': 1610254116495360.0, 'train_loss': 0.5193164943869597, 'epoch': 6.0})"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate(eval_dataset=test_dataset)\n",
        "print(\"K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\")\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "id": "sIUFQBCDcL9c",
        "outputId": "c36d2b73-799c-4649-c86f-85aedd690853"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='66' max='66' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [66/66 00:06]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "K·∫øt qu·∫£ ƒë√°nh gi√° tr√™n t·∫≠p test:\n",
            "{'eval_loss': 0.8318629264831543, 'eval_accuracy': 0.7095238095238096, 'eval_f1': 0.709959920505598, 'eval_runtime': 7.0442, 'eval_samples_per_second': 149.06, 'eval_steps_per_second': 9.369, 'epoch': 6.0}\n"
          ]
        }
      ]
    }
  ]
}